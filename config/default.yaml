# trainer:
  # trainer_cls: 'DQN'
  # policy: "MlpPolicy"     # MlpPolicy定义策略网络为MLP网络
  # learning_rate: 5e-4     # 学习速率
  # batch_size: 128         # 每次训练的batch大小
  # buffer_size: 50000      # replay buffer的大小
  # gamma: 0.9              # discount factor
  # learning_starts: 0
  # target_update_interval: 250
  # policy_kwargs: [256, 256]     # 这里代表隐藏层为2层256个节点数的网络
  # verbose: 0                   # verbose=1代表打印训练信息，如果是0为不打印，2为打印调试信息
  # tensorboard_log: "./tmp/tensorboard/"  # 训练数据保存目录，可以用tensorboard查看
  # total_timesteps: 1e4
  # progress_bar: True
  # device: "cuda"

  # trainer_cls: 'PPO'
  # policy: "MlpPolicy"                            # MlpPolicy定义策略网络为MLP网络
  # learning_rate: 5e-4
  # n_steps: 2048           # 运行N步后执行更新,buffer_size=n_steps*环境数量
  # batch_size: 128
  # n_epochs: 16             # 每次采用后训练次数
  # policy_kwargs: [256, 256]     # 这里代表隐藏层为2层256个节点数的网络
  # gamma: 0.99
  # verbose: 0            # verbose=1代表打印训练信息，如果是0为不打印，2为打印调试信息
  # tensorboard_log: "./tmp/tensorboard/"  # 训练数据保存目录，可以用tensorboard查看
  # total_timesteps: 1e4
  # progress_bar: True
  # device: "cuda"

  # trainer_cls: "CustomDQN"
  # lr: 2e-3
  # num_episodes: 100
  # hidden_dim : 128
  # gamma: 0.98
  # epsilon: 0.01
  # target_update: 10
  # buffer_size: 10000
  # minimal_size: 500
  # batch_size: 64
  # max_action_count: 10 # 最多重复选择动作次数，超过此次数则选择默认
  # device: "cuda"

  # where the training happens (cpu, cuda, cuda:0, ...)
  # note: rollouts are always collected using only the CPU
  # device: 'cuda'

  # name of the trainer's class
  # trainer_cls: 'DQN'

  # number of training iterations
  # num_iterations: 500

  # number of unique job sequences sampled per training iteration
  # num_sequences: 4

  # number of rollouts experienced per unique job sequence
  # `num_sequences` x `num_rollouts`
  #  = total number of rollouts per training iteration
  #  = number of rollout workers running in parallel
  # num_rollouts: 4

  # base random seed; each worker gets its own seed which is offset from this.
  # seed: 42

  # name of directory where all training artifacts are saved (e.g. tensorboard)
  # artifacts_dir: 'artifacts'

  # if checkpointing_freq = n, then every n iterations, the best model from the
  # past m iterations is saved
  # checkpointing_freq: 50

  # if true, then records training metrics to a tensorboard file
  # use_tensorboard: False

  # PPO: number of times to train through all of the data from the most recent
  # iteration
  # num_epochs: 3

  # PPO: number of batches to split the last iteration's training data into
  # num_batches: 10

  # PPO: hyperparameter for clamping the importance sampling ratio
  # clip_range: .2

  # PPO: end training cycle if approximate KL divergence exceeds `target_kl`
  # target_kl: .01

  # PPO: coefficient of entropy bonus term (if 0 then no entropy bonus)
  # entropy_coeff: .04

  # discount factor for (continuously) discounted returns
  # beta_discount: 5.e-3

  # max reward window size for differential returns
  # reward_buff_cap: 200000

  # note: only one of `beta_discount` and `reward_buff_cap` must be specified,
  # indicating whether to use discounted or differential returns

  # optimizer settings
  # opt_cls: 'Adam'
  # opt_kwargs: 
  #   lr: 3.e-4
  # max_grad_norm: .5


agent:
  agent_cls: 'DecimaScheduler'
  embed_dim: 16
  gnn_mlp_kwargs:
    hid_dims: [32, 16]
    act_cls: 'LeakyReLU'
    act_kwargs:
      inplace: True
      negative_slope: .2
  policy_mlp_kwargs:
    hid_dims: [64, 64]
    act_cls: 'Tanh'


# env:
  # id: "LayerEdgeEnv-v0"
  # num_executors: 50
  # job_arrival_cap: 200
  # job_arrival_rate: 4.e-5
  # moving_delay: 2000.
  # warmup_delay: 1000.
  # dataset: 'tpch'
  # mean_time_limit: 2.e+7

  # id: "FrozenLake-v1"
  # render_mode: "rgb_array"
  # map_name: "4x4"
  # is_slippery: false
  
  # id: "CartPole-v1"
  # id: "CliffWalking-v0"
  # id: "MyWrapper-v0"


###################### example1 ##########
# trainer:
#   trainer_cls: 'DQN'
#   policy: "MlpPolicy"                            # MlpPolicy定义策略网络为MLP网络
#   learning_rate: 1e-3
#   batch_size: 64
#   buffer_size: 10000
#   learning_starts: 500
#   target_update_interval: 1000
#   policy_kwargs:  {}    # 这里代表隐藏层为2层256个节点数的网络
#   verbose: 0                   # verbose=1代表打印训练信息，如果是0为不打印，2为打印调试信息
#   tensorboard_log: "./tmp/tensorboard/"  # 训练数据保存目录，可以用tensorboard查看
#   total_timesteps: 1e4
#   progress_bar: True
#   tau: 0.8  #软更新的比例,1就是硬更新
#   gamma: 0.9
#   train_freq: [1, 'step'] # 训练频率
#   device: "cpu"

# env:
#   id: "MyWrapper-v0"



############## example 2 #############
# trainer:
#   trainer_cls: 'PPO'
#   policy: "MlpPolicy"
#   total_timesteps: 2e4
#   progress_bar: True

# env:
#   id: "CartPole-v1"


############## example 3  DQN做layer-awary调度 #############
# trainer:
#   trainer_cls: 'DQN'
#   policy: "MlpPolicy"
#   learning_rate: 1e-3
#   batch_size: 512
#   gamma: 0.9
#   buffer_size: 50000
#   learning_starts: 0
#   target_update_interval: 1000
#   verbose: 0 
#   tensorboard_log: "./tmp/tensorboard/"
#   total_timesteps: 2e6
#   progress_bar: True
#   device: "cuda"
#   policy_kwargs: 
#     net_arch: [256, 256]
#     features_extractor_class: 'LayerDependentExtractor'
#     features_extractor_kwargs: 
#       'features_dim': 256 
#       'actions_dim': 6 # 5台机器+一台云服务器
#       'nodes_net_arch': [512, 256, 128]
#       'tasks_net_arch': [128, 128, 128]
# env:
#   id: "LayerEdgeEnv-v0"

########## example 4 layer depentdent ppo ############
trainer:
  trainer_cls: 'PPO'
  policy: "MlpPolicy"
  learning_rate: 1e-4
  n_steps: 512
  batch_size: 4096 # env_size * n_steps
  n_epochs: 32
  policy_kwargs:
    net_arch:
      policy_net: 
        node_layer: [256, 128] # 有n个输出. （这个其实是指定deepfm中的dnn结构）
        task_layer: [128, 64, 32]
        merge_layer: [32]
        N: 5   # 共有多少台边缘服务器
        L: 100  # 共有多少个层
      value_net: [128, 64, 32]
    # net_arch: [256, 256]
  gamma: 0.99
  verbose: 0 
  tensorboard_log: "./tmp/tensorboard/"
  total_timesteps: 2e6
  progress_bar: True
  device: "cpu"
env:
  id: "LayerEdgeEnv-v0"


############## normal ppo ###########3
# trainer:
#   trainer_cls: 'PPO'
#   policy: "MlpPolicy"
#   learning_rate: 1e-4
#   n_steps: 512
#   batch_size: 4096 # env_size * n_steps
#   n_epochs: 128
#   policy_kwargs:
#     net_arch: 
#       pi: [1024, 256, 64]
#       vf: [1024, 256, 64]
#   gamma: 0.99
#   verbose: 0 
#   tensorboard_log: "./tmp/tensorboard/"
#   total_timesteps: 5e5
#   progress_bar: True
#   device: "cuda"
# env:
#   id: "LayerEdgeEnv-v0"


############ normal dqn ###############
# trainer:
#   trainer_cls: 'DQN'
#   policy: "MlpPolicy"
#   learning_rate: 0.01
#   batch_size: 512
#   gamma: 0.9
#   buffer_size: 50000
#   learning_starts: 0
#   target_update_interval: 1000
#   verbose: 0 
#   tensorboard_log: "./tmp/tensorboard/"
#   total_timesteps: 2e5
#   progress_bar: True
#   device: "cuda"
#   policy_kwargs: 
#     net_arch: [256, 256]
# env:
#   id: "LayerEdgeEnv-v0"